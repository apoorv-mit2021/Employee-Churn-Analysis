# -*- coding: utf-8 -*-
"""DMPA_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vtYFraTJPwXkVe7Bd7KiAViwpnQ22U_k

#**Predicting Employee Churn with Decision Tree, Random Forests, KNN and Logistic Regression**
---

#Step 1: Importing Libraries
---
"""

# Instaliing pandas-profiling library

!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

# Commented out IPython magic to ensure Python compatibility.
# Importing the libraries

from __future__ import print_function
# %matplotlib inline
import os
import warnings
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as image
import pandas as pd
from pandas_profiling import ProfileReport
plt.style.use("ggplot")
plt.style.use("ggplot")
warnings.simplefilter("ignore")

plt.rcParams['figure.figsize'] = (12,8)

"""#Step 2: Exploratory Data Analysis
---
"""

# Importing the dataset

df = pd.read_csv('employee_data.csv')
df.head()

profile = ProfileReport(df, title = "Pandas Profiling Report", html = {"style":{'full_width':False}})

profile.to_notebook_iframe()

profile.to_file(output_file="REPORT.html")

"""#Step 3: Encode Categorical Features
---
"""

pd.crosstab(df.salary, df.quit).plot(kind='bar')
plt.title('Turnover Frequency on Salary Bracket')
plt.xlabel('Salary')
plt.ylabel('Frequency of Turnover')
plt.show()

pd.crosstab(df.department,df.quit).plot(kind='bar')
plt.title('Turnover Frequency for Department')
plt.xlabel('Department')
plt.ylabel('Frequency of Turnover')
plt.show()

# one hot encoding of categorical variables

cat_vars=['department','salary']
for var in cat_vars:
    cat_list='var'+'_'+var
    cat_list = pd.get_dummies(df[var], prefix=var)
    df1=df.join(cat_list)
    df=df1

df.columns

df.head()

# Dropping categorical variables

df.drop(columns=['department','salary'], axis=1, inplace=True)

"""# Step 4: Visualize Class Imbalance
---
"""

from yellowbrick.target import ClassBalance
plt.style.use("ggplot")
plt.rcParams['figure.figsize'] = (12,8)

visualizer = ClassBalance(labels=["stayed", "quit"])
visualizer.fit(df.quit)

"""# Step 5: Create Training and Test Sets
---
"""

X = df.loc[:, df.columns != 'quit']
y = df.quit

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0,
                                                    stratify=y)

"""# Step 6: Classification using Decision Tree Classifier
---
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics

# Training the Decision Tree Classification model on the Training set
model=DecisionTreeClassifier()
model=model.fit(X_train,y_train)

# Prediction 
y_pred=model.predict(X_test)

# Accuracy
print("\nAccuracy (Decision Tree Classifier Model):",metrics.accuracy_score(y_test, y_pred))

"""# Step 7: Build an Interactive Decision Tree Classifier
---
"""

from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.tree import export_graphviz # display the tree within notebook
from IPython.display import SVG
from graphviz import Source
from IPython.display import display
from ipywidgets import interactive, IntSlider, FloatSlider, interact
import ipywidgets
from IPython.display import Image
from subprocess import call
import matplotlib.image as mpimg

@interact
def plot_tree(crit=["gini", "entropy"],
              split=["best", "random"],
              depth=IntSlider(min=1,max=30,value=2, continuous_update=False),
              min_split=IntSlider(min=2,max=5,value=2, continuous_update=False),
              min_leaf=IntSlider(min=1,max=5,value=1, continuous_update=False)):
    
    estimator = DecisionTreeClassifier(random_state=0,
                                       criterion=crit,
                                       splitter = split,
                                       max_depth = depth,
                                       min_samples_split=min_split,
                                       min_samples_leaf=min_leaf)
    estimator.fit(X_train, y_train)
    print('Decision Tree Training Accuracy: {:.3f}'.format(accuracy_score(y_train, estimator.predict(X_train))))
    print('Decision Tree Test Accuracy: {:.3f}'.format(accuracy_score(y_test, estimator.predict(X_test))))

    graph = Source(tree.export_graphviz(estimator,
                                        out_file=None,
                                        feature_names=X_train.columns,
                                        class_names=['0', '1'],
                                        filled = True))
    
    display(Image(data=graph.pipe(format='png')))
    
    return estimator

"""#Step 8: Classification using Random Forest Cassifier
---
"""

# Training the Random Forest Classification model on the Training set

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

# Prediction
y_pred = classifier.predict(X_test)

# Accuracy 
print("\nAccuracy (Random Forest Classifier Model):",metrics.accuracy_score(y_test, y_pred))

"""# Step 9: Build an Interactive Random Forest Classifier
---
"""

@interact
def plot_tree_rf(crit=["gini", "entropy"],
                 bootstrap=["True", "False"],
                 depth=IntSlider(min=1,max=30,value=3, continuous_update=False),
                 forests=IntSlider(min=1,max=200,value=100,continuous_update=False),
                 min_split=IntSlider(min=2,max=5,value=2, continuous_update=False),
                 min_leaf=IntSlider(min=1,max=5,value=1, continuous_update=False)):
    
    estimator = RandomForestClassifier(random_state=1,
                                       criterion=crit,
                                       bootstrap=bootstrap,
                                       n_estimators=forests,
                                       max_depth=depth,
                                       min_samples_split=min_split,
                                       min_samples_leaf=min_leaf,
                                       n_jobs=-1,
                                      verbose=False).fit(X_train, y_train)

    print('Random Forest Training Accuracy: {:.3f}'.format(accuracy_score(y_train, estimator.predict(X_train))))
    print('Random Forest Test Accuracy: {:.3f}'.format(accuracy_score(y_test, estimator.predict(X_test))))
    num_tree = estimator.estimators_[0]
    print('\nVisualizing Decision Tree:', 0)
    
    graph = Source(tree.export_graphviz(num_tree,
                                        out_file=None,
                                        feature_names=X_train.columns,
                                        class_names=['0', '1'],
                                        filled = True))
    
    display(Image(data=graph.pipe(format='png')))
    
    return estimator

"""# Step 10: Classification using KNN classifier
---
"""

# Training the K-NN model on the Training set
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)

# Prediction
y_pred = classifier.predict(X_test)

# Accuracy 
print("\nAccuracy (Random Forest Classifier Model):",metrics.accuracy_score(y_test, y_pred))

"""#Step 11: Comparison with Logistic Regression Classifier
---
"""

from sklearn.linear_model import LogisticRegressionCV

logit = LogisticRegressionCV(random_state=1, n_jobs=-1,max_iter=500,
                             cv=10)

lr = logit.fit(X_train, y_train)

print('Logistic Regression Accuracy: {:.3f}'.format(accuracy_score(y_test, lr.predict(X_test))))